{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/quora-question-pairs/train.csv\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['is_duplicate'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df_train.dropna(how=\"any\").reset_index(drop=True)\na=0\nfor i in range(a,a+10):\n    print(df.question1[i])\n    print(df.question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer\nstop_words=set(stopwords.words('english'))\nquestion1= 'What would a Trump presidency mean for current international master’s students on an F1 visa?'\nquestion2='How will a Trump presidency affect the students presently in US or planning to study in US?'\nquestion1=question1.lower().split()\nquestion2=question2.lower().split()\nlancaster=LancasterStemmer()\nquestion1=[ lancaster.stem(w) for w in question1 if w not in stop_words]\nquestion2=[ lancaster.stem(w) for w in question2 if w not in stop_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim import models\nfrom gensim.models import Word2Vec\nmodel=models.KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distance=model.wmdistance(question1,question2)\nprint(distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.init_sims(replace=True)\ndistance=model.wmdistance(question1,question2)\nprint(distance)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question3 = 'Why am I mentally very lonely? How can I solve it?'\nquestion4 = 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?'\nquestion3 = question3.lower().split()\nquestion4 = question4.lower().split()\nquestion3 = [w for w in question3 if w not in stop_words]\nquestion4 = [w for w in question4 if w not in stop_words]\ndistance = model.wmdistance(question3, question4)\nprint('distance = %.4f' % distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.init_sims(replace=True)\ndistance = model.wmdistance(question3, question4)\nprint('normalized distance = %.4f' % distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz\nquestion1 = 'What would a Trump presidency mean for current international master’s students on an F1 visa?'\nquestion2 = 'How will a Trump presidency affect the students presently in US or planning to study in US?'\nfuzz.ratio(question1,question2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_token_set_ratio(question1,question2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question3 = 'Why am I mentally very lonely? How can I solve it?'\nquestion4 = 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?'\nfuzz.ratio(question3, question4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_token_set_ratio(question3, question4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wmd(q1, q2):\n    q1 = str(q1).lower().split()\n    q2 = str(q2).lower().split()\n    stop_words = stopwords.words('english')\n    q1 = [w for w in q1 if w not in stop_words]\n    q2 = [w for w in q2 if w not in stop_words]\n    return model.wmdistance(q1, q2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm_wmd(q1, q2):\n    q1 = str(q1).lower().split()\n    q2 = str(q2).lower().split()\n    stop_words = stopwords.words('english')\n    q1 = [w for w in q1 if w not in stop_words]\n    q2 = [w for w in q2 if w not in stop_words]\n    return norm_model.wmdistance(q1, q2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndef sent2vec(s):\n    words=str(s).lower()\n    words=word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words=[w for w in words if w.isalpha()]\n    m=[]\n    for w in words:\n        try:\n            m.append(model[w])\n        except:\n            continue\n    m=np.array(m)\n    v=m.sum(axis=0)\n    return v/np.sqrt((v**2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['len_q1']=df.question1.apply(lambda x: len(str(x)))\ndf['len_q2']=df.question2.apply(lambda x: len(str(x)))\ndf['diff_len']=df['len_q1']-df['len_q2']\ndf['len_char_q1']=df.question1.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\ndf['len_char_q2']=df.question2.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\ndf['len_word_q1'] = df.question1.apply(lambda x: len(str(x).split()))\ndf['len_word_q2'] = df.question2.apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['common_words'] = df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\ndf['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['wmd']=df.apply(lambda x: wmd(x['question1'],x['question2']),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_model = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\nnorm_model.init_sims(replace=True)\ndf['norm_wmd']=df.apply(lambda x:norm_wmd(x['question1'],x['question2']),axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\nquestion1_vectors = np.zeros((df.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df.question1.values)):\n    question1_vectors[i, :] = sent2vec(q)\n    \nquestion2_vectors  = np.zeros((df.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df.question2.values)):\n    question2_vectors[i, :] = sent2vec(q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nfrom scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\ndf['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\ndf['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\ndf['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\ndf['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['question1', 'question2'], axis=1, inplace=True)\ndf = df[pd.notnull(df['cosine_distance'])]\ndf = df[pd.notnull(df['jaccard_distance'])]\n\nX = df.loc[:, df.columns != 'is_duplicate']\ny = df.loc[:, df.columns == 'is_duplicate']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train.values.ravel()) \nprediction = model.predict(X_test)\ncm = confusion_matrix(y_test, prediction)  \nprint(cm)  \nprint('Accuracy', accuracy_score(y_test, prediction))\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Another effective method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv')\ndf.dropna(axis=0, inplace=True)\ndf.groupby(\"is_duplicate\")['id'].count().plot.bar()\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','qid1','qid2'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SPECIAL_TOKENS = {\n    'quoted': 'quoted_item',\n    'non-ascii': 'non_ascii_word',\n    'undefined': 'something'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SPECIAL_TOKENS = {\n    'quoted': 'quoted_item',\n    'non-ascii': 'non_ascii_word',\n    'undefined': 'something'\n}\n\ndef clean(text, stem_words=True):\n    import re\n    from string import punctuation\n    from nltk.stem import SnowballStemmer\n    from nltk.corpus import stopwords\n    \n    def pad_str(s):\n        return ' '+s+' '\n    \n    if pd.isnull(text):\n        return ''\n\n#    stops = set(stopwords.words(\"english\"))\n    # Clean the text, with the option to stem words.\n    \n    # Empty question\n    \n    if type(text) != str or text=='':\n        return ''\n\n    # Clean the text\n    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\'ve\", \" have \", text)\n    text = re.sub(\"can't\", \"can not\", text)\n    text = re.sub(\"n't\", \" not \", text)\n    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\'re\", \" are \", text)\n    text = re.sub(\"\\'d\", \" would \", text)\n    text = re.sub(\"\\'ll\", \" will \", text)\n    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n    \n    # remove comma between numbers, i.e. 15,000 -> 15000\n    \n    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n    \n#     # all numbers should separate from words, this is too aggressive\n    \n#     def pad_number(pattern):\n#         matched_string = pattern.group(0)\n#         return pad_str(matched_string)\n#     text = re.sub('[0-9]+', pad_number, text)\n    \n    # add padding to punctuations and special chars, we still need them later\n    \n    text = re.sub('\\$', \" dollar \", text)\n    text = re.sub('\\%', \" percent \", text)\n    text = re.sub('\\&', \" and \", text)\n    \n#    def pad_pattern(pattern):\n#        matched_string = pattern.group(0)\n#       return pad_str(matched_string)\n#    text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text) \n        \n    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n    \n    # indian dollar\n    \n    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n    \n    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" india \", \" India \", text)\n    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n    text = re.sub(r\" china \", \" China \", text)\n    text = re.sub(r\" chinese \", \" Chinese \", text) \n    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" III \", \" 3 \", text)\n    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n    \n    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n    \n    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 0 \nfor i in range(a,a+10):\n    print(df.question1[i])\n    print(df.question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from scipy import CountVectorized\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect=CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}')\ncount_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainq1_trans=count_vect.transform(df['question1'].values)\ntrainq2_trans=count_vect.transform(df['question2'].values)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=df['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=scipy.sparse.hstack((trainq1_trans,trainq2_trans))\ny=labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=[1,2,3]\nb=[2,3]\nscipy.sparse.hstack([a,b]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgb_model=xgb.XGBClassifier(max_depth=50,n_estimator=80,colsample_bytree=.7,gamma=0,reg_alpha=4,objective='binary:logistic',eta=0.3,silent=1,subsample=0.8).fit(X_train,y_train)\nxgb_prediction=xgb_model.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report,accuracy_score\nprint('train score',f1_score(y_train,xgb_model.predict(X_train)))\nprint('validation score',f1_score(y_valid,xgb_model.predict(X_valid)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_valid,xgb_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(pd.concat((df['question1'],df['question2'])).unique())\ntrainq1_trans = tfidf_vect.transform(df['question1'].values)\ntrainq2_trans = tfidf_vect.transform(df['question2'].values)\nlabels = df['is_duplicate'].values\nX = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\ny = labels\nX_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n\nxgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \nxgb_prediction = xgb_model.predict(X_valid)\nprint('word level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\nprint('word level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\nprint(classification_report(y_valid, xgb_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfq1, dfq2 = df[['qid1', 'question1']], df[['qid2', 'question2']]\ndfq1.columns=['qid1','question']\ndfq1.columns=['qid2','question']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge two two dfs, there are two nans for question\ndfqa = pd.concat((dfq1, dfq2), axis=0).fillna(\"\")\nnrows_for_q1 = dfqa.shape[0]/2\ndfqa.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nmq1=TfidfVectorizer(max_features=256,stop_words='english').fit_transform(dfqa['question'].values)\nmq1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport pandas as pd\n\nimport matplotlib\n\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.models import Model, Sequential\nfrom tensorflow.python.keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n\nfrom utilsim import make_w2v_embeddings\nfrom utilsim import split_and_zero_padding\nfrom utilsim import ManDist\n\n# File paths\nTRAIN_CSV = '/kaggle/input/quora-question-pairs/train.csv'\n\n# Load training set\ntrain_df = pd.read_csv(TRAIN_CSV)\nfor q in ['question1', 'question2']:\n    train_df[q + '_n'] = train_df[q]\n\n# Make word2vec embeddings\nembedding_dim = 300\nmax_seq_length = 20\nuse_w2v = True\n\ntrain_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n\n# Split to train validation\nvalidation_size = int(len(train_df) * 0.1)\ntraining_size = len(train_df) - validation_size\n\nX = train_df[['question1_n', 'question2_n']]\nY = train_df['is_duplicate']\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n\nX_train = split_and_zero_padding(X_train, max_seq_length)\nX_validation = split_and_zero_padding(X_validation, max_seq_length)\n\n# Convert labels to their numpy representations\nY_train = Y_train.values\nY_validation = Y_validation.values\n\n# Make sure everything is ok\nassert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)\n\n# --\n\n# Model variables\ngpus = 2\nbatch_size = 1024 * gpus\nn_epoch = 50\nn_hidden = 50\n\n# Define the shared model\nx = Sequential()\nx.add(Embedding(len(embeddings), embedding_dim,\n                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n# CNN\n# x.add(Conv1D(250, kernel_size=5, activation='relu'))\n# x.add(GlobalMaxPool1D())\n# x.add(Dense(250, activation='relu'))\n# x.add(Dropout(0.3))\n# x.add(Dense(1, activation='sigmoid'))\n# LSTM\nx.add(LSTM(n_hidden))\n\nshared_model = x\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\n# Pack it all up into a Manhattan Distance model\nmalstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\nmodel = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n\nif gpus >= 2:\n    # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n    model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\nmodel.summary()\nshared_model.summary()\n\n# Start trainings\ntraining_start_time = time()\nmalstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n                           batch_size=batch_size, epochs=n_epoch,\n                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\ntraining_end_time = time()\nprint(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n                                                        training_end_time - training_start_time))\n\nmodel.save('./data/SiameseLSTM.h5')\n\n# Plot accuracy\nplt.subplot(211)\nplt.plot(malstm_trained.history['acc'])\nplt.plot(malstm_trained.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\n# Plot loss\nplt.subplot(212)\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.tight_layout(h_pad=1.0)\nplt.savefig('./data/history-graph.png')\n\nprint(str(malstm_trained.history['val_acc'][-1])[:6] +\n      \"(max: \" + str(max(malstm_trained.history['val_acc']))[:6] + \")\")\nprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nfrom nltk.corpus import stopwords\nfrom gensim.models import KeyedVectors\n\nimport gensim\n\nimport numpy as np\n\nimport itertools\n\n\ndef text_to_word_list(text):\n    # Pre process and convert texts to a list of words\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n\ndef make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n    vocabs = {}\n    vocabs_cnt = 0\n\n    vocabs_not_w2v = {}\n    vocabs_not_w2v_cnt = 0\n\n    # Stopwords\n    stops = set(stopwords.words('english'))\n\n    # Load word2vec\n    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n\n    if empty_w2v:\n        word2vec = EmptyWord2Vec\n    else:\n        word2vec = KeyedVectors.load_word2vec_format(\"./data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n\n    for index, row in df.iterrows():\n        # Print the number of embedded sentences.\n        if index != 0 and index % 1000 == 0:\n            print(\"{:,} sentences embedded.\".format(index), flush=True)\n\n        # Iterate through the text of both questions of the row\n        for question in ['question1', 'question2']:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n                # Check for unwanted words\n                if word in stops:\n                    continue\n\n                # If a word is missing from word2vec model.\n                if word not in word2vec.vocab:\n                    if word not in vocabs_not_w2v:\n                        vocabs_not_w2v_cnt += 1\n                        vocabs_not_w2v[word] = 1\n\n                # If you have never seen a word, append it to vocab dictionary.\n                if word not in vocabs:\n                    vocabs_cnt += 1\n                    vocabs[word] = vocabs_cnt\n                    q2n.append(vocabs_cnt)\n                else:\n                    q2n.append(vocabs[word])\n\n            # Append question as number representation\n            df.at[index, question + '_n'] = q2n\n\n    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n    embeddings[0] = 0  # So that the padding will be ignored\n\n    # Build the embedding matrix\n    for word, index in vocabs.items():\n        if word in word2vec.vocab:\n            embeddings[index] = word2vec.word_vec(word)\n    del word2vec\n\n    return df, embeddings\n\n\ndef split_and_zero_padding(df, max_seq_length):\n    # Split to dicts\n    X = {'left': df['question1_n'], 'right': df['question2_n']}\n\n    # Zero padding\n    for dataset, side in itertools.product([X], ['left', 'right']):\n        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n\n    return dataset\n\n\n#  --\n\nclass ManDist(Layer):\n    \"\"\"\n    Keras Custom Layer that calculates Manhattan Distance.\n    \"\"\"\n\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer's logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)\n\n\nclass EmptyWord2Vec:\n    \"\"\"\n    Just for test use.\n    \"\"\"\n    vocab = {}\nword_vec = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, LSTM, Lambda\nimport keras.backend as K\nfrom keras.optimizers import Adadelta\nfrom keras.callbacks import ModelCheckpoint\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_CSV = '/kaggle/input/quora-question-pairs/train.csv'\nTEST_CSV = '/kaggle/input/quora-question-pairs/test.csv'\nEMBEDDING_FILE = '/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\nMODEL_SAVING_DIR = '/home/ecohen/HDD/HDD4/Models/Kaggle/Quora/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/quora-question-pairs/train.csv')\ntest_df=pd.read_csv('/kaggle/input/quora-question-pairs/test.csv')\n\nprint(len(train_df))\nprint(len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# traindf=[]\nnp.random.seed(seed=1) #makes result reproducible\nmsk = np.random.rand(len(train_df)) < 0.4\ntrain_df = train_df[msk]\nmsk = np.random.rand(len(test_df)) < 0.1\ntest_df = test_df[msk]\n# evaldf = df[~msk]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_df))\nprint(len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load training and test set\n# train_df = pd.read_csv(TRAIN_CSV)\n# test_df = pd.read_csv(TEST_CSV)\n\nstops = set(stopwords.words('english'))\n\ndef text_to_word_list(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    text = text.split()\n\n    return text\n\n# Prepare embedding\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nquestions_cols = ['question1', 'question2']\n\n# Iterate over the questions only of both training and test datasets\nfor dataset in [train_df, test_df]:\n    for index, row in dataset.iterrows():\n\n        # Iterate through the text of both questions of the row\n        for question in questions_cols:\n\n            q2n = []  # q2n -> question numbers representation\n            for word in text_to_word_list(row[question]):\n\n                # Check for unwanted words\n                if word in stops and word not in word2vec.vocab:\n                    continue\n\n                if word not in vocabulary:\n                    vocabulary[word] = len(inverse_vocabulary)\n                    q2n.append(len(inverse_vocabulary))\n                    inverse_vocabulary.append(word)\n                else:\n                    q2n.append(vocabulary[word])\n\n            # Replace questions as word to question as number representation\n            dataset.set_value(index, question, q2n)\n            \nembedding_dim = 300\nembeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\nembeddings[0] = 0  # So that the padding will be ignored\n\n# Build the embedding matrix\nfor word, index in vocabulary.items():\n    if word in word2vec.vocab:\n        embeddings[index] = word2vec.word_vec(word)\n\ndel word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n                     train_df.question2.map(lambda x: len(x)).max(),\n                     test_df.question1.map(lambda x: len(x)).max(),\n                     test_df.question2.map(lambda x: len(x)).max())\n\n# Split to train validation\nvalidation_size = 4000\ntraining_size = len(train_df) - validation_size\n\nX = train_df[questions_cols]\nY = train_df['is_duplicate']\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n\n# Split to dicts\nX_train = {'left': X_train.question1, 'right': X_train.question2}\nX_validation = {'left': X_validation.question1, 'right': X_validation.question2}\nX_test = {'left': test_df.question1, 'right': test_df.question2}\n\n# Convert labels to their numpy representations\nY_train = Y_train.values\nY_validation = Y_validation.values\n\n# Zero padding\nfor dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n\n# Make sure everything is ok\nassert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)\n\n\n\n# Model variables\nn_hidden = 50\ngradient_clipping_norm = 1.25\nbatch_size = 64\nn_epoch = 10\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# The visible layer\nleft_input = Input(shape=(max_seq_length,), dtype='int32')\nright_input = Input(shape=(max_seq_length,), dtype='int32')\n\nembedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n\n# Embedded version of the inputs\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Since this is a siamese network, both sides share the same LSTM\nshared_lstm = LSTM(n_hidden)\n\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n\n# Pack it all up into a model\nmalstm = Model([left_input, right_input], [malstm_distance])\n\n# Adadelta optimizer, with gradient clipping by norm\noptimizer = Adadelta(clipnorm=gradient_clipping_norm)\n\nmalstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n\n# Start training\ntraining_start_time = time()\n\nmalstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(malstm_trained.history['acc'])\nplt.plot(malstm_trained.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot loss\nplt.plot(malstm_trained.history['loss'])\nplt.plot(malstm_trained.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"malstm.save(\"similar.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"malstm.predict(\"this is cow\",\"cow is this\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}