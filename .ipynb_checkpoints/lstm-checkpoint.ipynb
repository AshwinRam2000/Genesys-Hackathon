{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13019884996816672404, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3148847513\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 7735819955652830624\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:702: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>correct</th>\n",
       "      <th>sno.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>A company is incorporated in a specific nation...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Today, there is a growing community of more th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Corporation definition, an association of indi...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Examples of corporation in a Sentence. 1  He w...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>1: a government-owned corporation (as a utilit...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sno                  question  \\\n",
       "0    0  . what is a corporation?   \n",
       "1    0  . what is a corporation?   \n",
       "2    0  . what is a corporation?   \n",
       "3    0  . what is a corporation?   \n",
       "4    0  . what is a corporation?   \n",
       "\n",
       "                                              answer  correct  sno.1  \n",
       "0  A company is incorporated in a specific nation...        0      0  \n",
       "1  Today, there is a growing community of more th...        0      1  \n",
       "2  Corporation definition, an association of indi...        0      2  \n",
       "3  Examples of corporation in a Sentence. 1  He w...        0      3  \n",
       "4  1: a government-owned corporation (as a utilit...        0      4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data1=pd.read_csv(\"data.tsv\", sep='\\t',names=[\"sno\",\"question\",\"answer\",\"correct\",\"sno\"])\n",
    "len(list(data1[\"answer\"]))\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try[\"com\"]=data_try[\"question\"]+\" \"+data_try[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(\" \") for doc in data_try[\"com\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_matrix(tokenized_corpus, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 11305)\n"
     ]
    }
   ],
   "source": [
    "encoded_docs.shape\n",
    "import keras \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(encoded_docs)\n",
    "print(X.shape)\n",
    "data_try['sentiment'] = ['pos' if (x==1) else 'neg' for x in data_try['correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Today there is a growing community of more than 2100 Certified B Corps from 50 countries and over 130 industries working together toward 1 unifying goal to redefine success in business Join the Movement'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#I have considered a rating above 3 as positive and less than or equal to 3 as negative.\n",
    "data_try['sentiment'] = ['pos' if (x==1) else 'neg' for x in data_try['correct']]\n",
    "data_try['com'] = data_try['com'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "# for idx,row in data_try.iterrows():\n",
    "#     row[0] = row[0].replace('rt',' ')\n",
    "data_try['com'] = [x.encode('ascii') for x in data_try['com']]\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=2500, lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(data_try['com'][1])\n",
    "#print(tokenizer.word_index)  # To see the dicstionary\n",
    "X = tokenizer.texts_to_sequences(data_try['com'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 1757, 128)         320000    \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 583,602\n",
      "Trainable params: 583,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "# model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(10, 248)))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Y = pd.get_dummies(data_try['sentiment']).values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "Y=encoder.fit_transform(data_try['sentiment'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y,test_size = 0.20, random_state = 36)\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size =32, nb_epoch = 1,  verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,ev=model.evaluate(X_valid,Y_valid,verbose=2,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(\" \") for doc in \"hi how are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_docs = t.texts_to_matrix(tokenized_corpus, mode='count')\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 11305, 128)        320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 400)               526400    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 846,801\n",
      "Trainable params: 846,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "# model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(10, 248)))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1757)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
