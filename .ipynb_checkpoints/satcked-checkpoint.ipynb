{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 13019884996816672404, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3148847513\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 7735819955652830624\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashwin Ram\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:702: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>correct</th>\n",
       "      <th>sno.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>A company is incorporated in a specific nation...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Today, there is a growing community of more th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Corporation definition, an association of indi...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>Examples of corporation in a Sentence. 1  He w...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>. what is a corporation?</td>\n",
       "      <td>1: a government-owned corporation (as a utilit...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sno                  question  \\\n",
       "0    0  . what is a corporation?   \n",
       "1    0  . what is a corporation?   \n",
       "2    0  . what is a corporation?   \n",
       "3    0  . what is a corporation?   \n",
       "4    0  . what is a corporation?   \n",
       "\n",
       "                                              answer  correct  sno.1  \n",
       "0  A company is incorporated in a specific nation...        0      0  \n",
       "1  Today, there is a growing community of more th...        0      1  \n",
       "2  Corporation definition, an association of indi...        0      2  \n",
       "3  Examples of corporation in a Sentence. 1  He w...        0      3  \n",
       "4  1: a government-owned corporation (as a utilit...        0      4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data1=pd.read_csv(\"data.tsv\", sep='\\t',names=[\"sno\",\"question\",\"answer\",\"correct\",\"sno\"])\n",
    "len(list(data1[\"answer\"]))\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_try[\"com\"]=data_try[\"question\"]+\" \"+data_try[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [doc.split(\" \") for doc in data_try[\"com\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_matrix(tokenized_corpus, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 11305)\n"
     ]
    }
   ],
   "source": [
    "encoded_docs.shape\n",
    "import keras \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(encoded_docs)\n",
    "print(X.shape)\n",
    "data_try['sentiment'] = ['pos' if (x==1) else 'neg' for x in data_try['correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Bi_LSTM(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
    "    speed_input = Input(shape = (X.shape[1], X.shape[2]), name = 'speed')\n",
    "    \n",
    "    main_output = Bidirectional(LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2], return_sequences=False), merge_mode='ave')(speed_input)\n",
    "    \n",
    "    final_model = Model(input = [speed_input], output = [main_output])\n",
    "    \n",
    "    final_model.summary()\n",
    "    \n",
    "    final_model.compile(loss='mse', optimizer='rmsprop')\n",
    "    \n",
    "    history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
    "    final_model.fit([X], Y, validation_split = 0.2, nb_epoch = epochs, callbacks=[history, earlyStopping])\n",
    "    \n",
    "    return final_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2_Bi_LSTM_mask(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0.,input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(LSTM(output_dim = X.shape[2], return_sequences=True, input_shape = (X.shape[1], X.shape[2])))\n",
    "    model.add(LSTM(output_dim = X.shape[2], return_sequences=False, input_shape = (X.shape[1], X.shape[2])))\n",
    "\n",
    "    model.add(Dense(X.shape[2]))\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "    history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
    "    model.fit(X, Y, validation_split = 0.2, nb_epoch = epochs, callbacks=[history, earlyStopping])\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def train_2_Bi_LSTM(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
    "    speed_input = Input(shape = (X.shape[1], X.shape[2]), name = 'speed')\n",
    "    \n",
    "    lstm_output = Bidirectional(LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2], return_sequences=True), merge_mode='ave')(speed_input)\n",
    "    \n",
    "    main_output = LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2])(lstm_output)\n",
    "    \n",
    "    final_model = Model(input = [speed_input], output = [main_output])\n",
    "    \n",
    "    final_model.summary()\n",
    "    \n",
    "    final_model.compile(loss='mse', optimizer='rmsprop')\n",
    "    \n",
    "    history = LossHistory()\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
    "    final_model.fit([X], Y, validation_split = 0.2, nb_epoch = epochs, callbacks=[history, earlyStopping])\n",
    "    \n",
    "    return final_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
